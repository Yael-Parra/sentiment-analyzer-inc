from dotenv import load_dotenv
import os, requests, time, pandas as pd
import re

load_dotenv()
API_KEY = os.getenv("YouTube_Data_API_v3")

def extract_video_id(url_or_id):
    print(f"üîç Extrayendo ID del v√≠deo de: {url_or_id}")
    pattern = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(pattern, url_or_id)
    if match:
        video_id = match.group(1)
        print(f"‚úÖ ID extra√≠do: {video_id}")
        return video_id
    else:
        print(f"‚ö†Ô∏è No se pudo extraer ID, asumiendo input es ID directo: {url_or_id}")
        return url_or_id

def fetch_comment_threads(video_id, max_total=100000, delay=1):
    print(f"‚ñ∂Ô∏è Iniciando extracci√≥n de comentarios para video {video_id}")
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    comments = []
    token = None
    total = 0
    round_count = 0

    while total < max_total:
        round_count += 1
        print(f"\nüîÅ Petici√≥n {round_count}: descargando comentarios...")
        params = {
            "part": "snippet,replies",
            "videoId": video_id,
            "maxResults": min(100, max_total - total),
            "textFormat": "plainText",
            "key": API_KEY
        }
        if token:
            params["pageToken"] = token
            print(f"‚û°Ô∏è Usando pageToken: {token}")

        response = requests.get(url, params=params)
        print(f"üì¶ Estado HTTP comentarios: {response.status_code}")
        response.raise_for_status()
        data = response.json()
        items = data.get("items", [])
        print(f"üì• Comentarios recibidos en esta tanda: {len(items)}")

        for item in items:
            s = item["snippet"]
            top = s["topLevelComment"]["snippet"]

            comments.append({
                "threadId": item["id"],
                "commentId": item["id"],
                "videoId": video_id,
                "author": top.get("authorDisplayName"),
                "authorChannelId": top.get("authorChannelId", {}).get("value"),
                "isReply": False,
                "parentCommentId": None,
                "publishedAtComment": top.get("publishedAt"),
                "text": top.get("textDisplay"),
                "likeCountComment": top.get("likeCount"),
                "replyCount": s.get("totalReplyCount")
            })
            total += 1

            for rep in item.get("replies", {}).get("comments", []):
                rps = rep["snippet"]
                comments.append({
                    "threadId": item["id"],
                    "commentId": rep["id"],
                    "videoId": video_id,
                    "author": rps.get("authorDisplayName"),
                    "authorChannelId": rps.get("authorChannelId", {}).get("value"),
                    "isReply": True,
                    "parentCommentId": rps.get("parentId"),
                    "publishedAtComment": rps.get("publishedAt"),
                    "text": rps.get("textDisplay"),
                    "likeCountComment": rps.get("likeCount"),
                    "replyCount": None
                })
                total += 1
                if total >= max_total:
                    break
            if total >= max_total:
                break

        print(f"‚úÖ Total comentarios acumulados: {total}")
        token = data.get("nextPageToken")
        if not token:
            print("üö´ No hay m√°s p√°ginas disponibles.")
            break

        print(f"‚è≥ Esperando {delay} segundos antes de la siguiente petici√≥n...")
        time.sleep(delay)

    print(f"\nüéØ Total final de comentarios extra√≠dos: {total}")
    return comments

if __name__ == "__main__":
    url_or_id = input("Introduce URL o ID de v√≠deo YouTube: ").strip()
    video_id = extract_video_id(url_or_id)

    print("Extrayendo comentarios y respuestas...")
    comments = fetch_comment_threads(video_id, max_total=100000)
    print(f"{len(comments)} comentarios y respuestas extra√≠dos.")

    df = pd.DataFrame(comments)
    print("Primeros comentarios extra√≠dos:")
    print(df.head())
