from dotenv import load_dotenv
import os, requests, time, pandas as pd
import re

load_dotenv()
API_KEY = os.getenv("YouTube_Data_API_v3")

def extract_video_id(url_or_id):
    print(f"üîç Extrayendo ID del v√≠deo de: {url_or_id}")
    pattern = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(pattern, url_or_id)
    if match:
        video_id = match.group(1)
        print(f"‚úÖ ID extra√≠do: {video_id}")
        return video_id
    else:
        print(f"‚ö†Ô∏è No se pudo extraer ID, asumiendo input es ID directo: {url_or_id}")
        return url_or_id

def fetch_video_metadata(video_id, api_key):
    print(f"‚ñ∂Ô∏è Iniciando extracci√≥n de metadatos para video {video_id}")
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {
        "part": "snippet,statistics",
        "id": video_id,
        "key": api_key
    }
    r = requests.get(url, params=params)
    print(f"üì¶ Estado HTTP metadata: {r.status_code}")
    r.raise_for_status()
    items = r.json().get("items", [])
    if not items:
        raise ValueError("Video no encontrado")
    print(f"‚úÖ Metadatos obtenidos para video {video_id}")
    return items[0]

def fetch_comment_threads(video_id, max_total=100000, delay=1):
    print(f"‚ñ∂Ô∏è Iniciando extracci√≥n de comentarios para video {video_id}")
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    comments = []
    token = None
    total = 0
    round_count = 0

    while total < max_total:
        round_count += 1
        print(f"\nüîÅ Petici√≥n {round_count}: descargando comentarios...")
        params = {
            "part": "snippet,replies",
            "videoId": video_id,
            "maxResults": min(100, max_total - total),
            "textFormat": "plainText",
            "key": API_KEY
        }
        if token:
            params["pageToken"] = token
            print(f"‚û°Ô∏è Usando pageToken: {token}")

        response = requests.get(url, params=params)
        print(f"üì¶ Estado HTTP comentarios: {response.status_code}")
        response.raise_for_status()
        data = response.json()
        items = data.get("items", [])
        print(f"üì• Comentarios recibidos en esta tanda: {len(items)}")

        for item in items:
            s = item["snippet"]
            top = s["topLevelComment"]["snippet"]

            comments.append({
                "threadId": item["id"],
                "commentId": item["id"],
                "videoId": video_id,
                "author": top.get("authorDisplayName"),
                "authorChannelId": top.get("authorChannelId", {}).get("value"),
                "isReply": False,
                "parentCommentId": None,
                "publishedAtComment": top.get("publishedAt"),
                "text": top.get("textDisplay"),
                "likeCountComment": top.get("likeCount"),
                "replyCount": s.get("totalReplyCount")
            })
            total += 1

            for rep in item.get("replies", {}).get("comments", []):
                rps = rep["snippet"]
                comments.append({
                    "threadId": item["id"],
                    "commentId": rep["id"],
                    "videoId": video_id,
                    "author": rps.get("authorDisplayName"),
                    "authorChannelId": rps.get("authorChannelId", {}).get("value"),
                    "isReply": True,
                    "parentCommentId": rps.get("parentId"),
                    "publishedAtComment": rps.get("publishedAt"),
                    "text": rps.get("textDisplay"),
                    "likeCountComment": rps.get("likeCount"),
                    "replyCount": None
                })
                total += 1
                if total >= max_total:
                    break
            if total >= max_total:
                break

        print(f"‚úÖ Total comentarios acumulados: {total}")
        token = data.get("nextPageToken")
        if not token:
            print("üö´ No hay m√°s p√°ginas disponibles.")
            break

        print(f"‚è≥ Esperando {delay} segundos antes de la siguiente petici√≥n...")
        time.sleep(delay)

    print(f"\nüéØ Total final de comentarios extra√≠dos: {total}")
    return comments

def save_results(df, video_metadata, outdir="etl/data"):
    print("‚ñ∂Ô∏è Guardando resultados...")
    os.makedirs(outdir, exist_ok=True)
    vid = video_metadata["id"]
    path_comments = os.path.join(outdir, f"youtube_comments_{vid}.csv.gz")
    df.to_csv(path_comments, index=False, compression="gzip", encoding="utf-8")
    
    video_info = {
        "videoId": vid,
        "videoPublishedAt": video_metadata["snippet"]["publishedAt"],
        "videoLikeCount": int(video_metadata["statistics"].get("likeCount", 0)),
        "videoCommentCount": int(video_metadata["statistics"].get("commentCount", 0))
    }
    meta_df = pd.DataFrame([video_info])
    path_meta = os.path.join(outdir, f"youtube_metadata_{vid}.csv")
    meta_df.to_csv(path_meta, index=False)
    print(f"‚úÖ Comentarios guardados en: {path_comments}")
    print(f"‚úÖ Metadatos guardados en: {path_meta}")

if __name__ == "__main__":
    url_or_id = input("Introduce URL o ID de v√≠deo YouTube: ").strip()
    video_id = extract_video_id(url_or_id)
    if not API_KEY:
        print("‚ùå No API_KEY en .env")
        exit(1)

    print(f"Extrayendo metadatos del v√≠deo {video_id}...")
    metadata = fetch_video_metadata(video_id, API_KEY)
    print("Extrayendo comentarios y respuestas...")
    comments = fetch_comment_threads(video_id, max_total=100000)
    print(f"{len(comments)} comentarios y respuestas extra√≠dos.")

    df = pd.DataFrame(comments)
    print("Primeros comentarios extra√≠dos:")
    print(df.head())

    save_results(df, metadata)
