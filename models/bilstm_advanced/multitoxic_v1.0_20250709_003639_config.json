{
  "model_config": {
    "vocab_size": 2785,
    "embedding_dim": 256,
    "hidden_dim": 192,
    "num_classes": 12,
    "num_numeric_features": 84,
    "num_layers": 2,
    "dropout_rate": 0.4,
    "max_sequence_length": 50
  },
  "classes": {
    "target_classes": [
      "is_toxic",
      "is_hatespeech",
      "is_abusive",
      "is_provocative",
      "is_racist",
      "is_obscene",
      "is_threat",
      "is_religious_hate",
      "is_nationalist",
      "is_sexist",
      "is_radicalism",
      "is_homophobic"
    ],
    "class_names": [
      "toxic",
      "hatespeech",
      "abusive",
      "provocative",
      "racist",
      "obscene",
      "threat",
      "religious_hate",
      "nationalist",
      "sexist",
      "homophobic",
      "radicalism"
    ],
    "num_classes": 12,
    "category_mapping": {
      "identity": [
        "racist",
        "sexist",
        "homophobic",
        "religious_hate",
        "nationalist"
      ],
      "behavior": [
        "abusive",
        "provocative",
        "threat",
        "radicalism"
      ],
      "content": [
        "obscene",
        "hatespeech"
      ],
      "general": [
        "toxic"
      ]
    }
  },
  "performance": {
    "test_metrics": {
      "f1_macro": 0.9516213087004624,
      "f1_micro": 0.9514975845410628,
      "f1_weighted": 0.953250362890349,
      "hamming_loss": 0.01473004650324583,
      "jaccard_score": 0.6822267174720764
    },
    "training_results": {
      "best_f1_macro": 0.9555352605886971,
      "best_loss": 0.04942152874751223,
      "epochs_trained": 60,
      "final_train_loss": 0.0314873720508741,
      "final_val_loss": 0.06563601882921324
    }
  },
  "thresholds": {
    "toxic": 0.5,
    "hatespeech": 0.4,
    "abusive": 0.4,
    "provocative": 0.6,
    "racist": 0.3,
    "obscene": 0.5,
    "threat": 0.2,
    "religious_hate": 0.3,
    "nationalist": 0.4,
    "sexist": 0.3,
    "homophobic": 0.3,
    "radicalism": 0.2
  },
  "dataset_info": {
    "total_samples": 7096,
    "train_samples": 4257,
    "val_samples": 1419,
    "test_samples": 1420,
    "features_count": 107,
    "vocab_size": 2785,
    "sequence_length": 50
  },
  "metadata": {
    "model_version": "multitoxic_v1.0_20250709_003639",
    "creation_date": "20250709_003639",
    "pytorch_version": "2.2.2",
    "total_parameters": 3133727,
    "trainable_parameters": 3133727,
    "architecture": "BiLSTM_Hybrid_MultiHead_Attention_Categorical",
    "problem_type": "multi_label_classification",
    "language": "English",
    "toxicity_types": 12,
    "spacy_model": "en_core_web_sm",
    "validation_method": "train_test_split"
  },
  "files": {
    "model": "multitoxic_v1.0_20250709_003639_model.pth",
    "processor": "multitoxic_v1.0_20250709_003639_processor.pkl",
    "features": "multitoxic_v1.0_20250709_003639_features.pkl",
    "config": "multitoxic_v1.0_20250709_003639_config.json",
    "loader": "multitoxic_v1.0_20250709_003639_loader.py"
  }
}