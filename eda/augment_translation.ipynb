{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <span style=\"color:#f6f794\"> Translation from EN to ES </span> </center>\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#f6f794\"> üìö Importing libraries and setting displays </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "import os\n",
    "import csv\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#f6f794\"> üìÇ Opening files and create dataframes of them </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yaelp\\Desktop\\Yael\\Bootcamp_FactoriaF5_IA_Promo4\\Proyectos\\Grupales\\sentiment-analyzer-inc\\eda\n",
      "c:\\Users\\yaelp\\Desktop\\Yael\\Bootcamp_FactoriaF5_IA_Promo4\\Proyectos\\Grupales\\sentiment-analyzer-inc\n",
      "c:\\Users\\yaelp\\Desktop\\Yael\\Bootcamp_FactoriaF5_IA_Promo4\\Proyectos\\Grupales\\sentiment-analyzer-inc\\eda\\data\n",
      "c:\\Users\\yaelp\\Desktop\\Yael\\Bootcamp_FactoriaF5_IA_Promo4\\Proyectos\\Grupales\\sentiment-analyzer-inc\\eda\\\n"
     ]
    }
   ],
   "source": [
    "# Setting paths\n",
    "current_dir = os.getcwd()                            # \\sentiment-analyzer-inc\\eda\n",
    "print(current_dir)\n",
    "parent_dir = os.path.dirname(current_dir)            # \\sentiment-analyzer-inc\n",
    "print(parent_dir)\n",
    "data_dir = os.path.join(current_dir, \"data\")         # \\sentiment-analyzer-inc\\eda\\data\n",
    "print(data_dir)\n",
    "cleaning_file_path = os.path.join(current_dir, \"\")   # \\sentiment-analyzer-inc\\eda\\data\\\n",
    "print(cleaning_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Downloading data from GitHub...\n",
      "üìä Processing compressed file..\n",
      "‚úÖ Data downloaded succesfully!\n"
     ]
    }
   ],
   "source": [
    "GITHUB_CLEAN_URL = \"https://raw.githubusercontent.com/Yael-Parra/sentiment-analyzer-inc/feature/eda/eda/data/youtube_dataset_cleaned.csv.gz\"\n",
    "\n",
    "\n",
    "def load_airbnb_data_from_github(url):\n",
    "    \"\"\"\n",
    "    Downloading and processing Airbnb data from GitHub repository.\n",
    "    \"\"\"\n",
    "    print(\"üîó Downloading data from GitHub...\")\n",
    "\n",
    "    try:\n",
    "        # Download the compressed CSV file from GitHub\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(\"üìä Processing compressed file..\")\n",
    "\n",
    "        # Decompressing and reading the CSV file\n",
    "        with gzip.open(BytesIO(response.content), 'rt', encoding='utf-8') as f:\n",
    "            df = pd.read_csv(\n",
    "                f,\n",
    "                sep=',',                  # Specify the delimiter\n",
    "                encoding='utf-8',         # Specify the encoding\n",
    "                quoting=csv.QUOTE_MINIMAL # Specify the quoting behavior\n",
    "                )\n",
    "\n",
    "        print(f\"‚úÖ Data downloaded succesfully!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"‚ùå Network/Request Error while downloading data: {req_err}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå General Error while processing data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Creating dataframe from GitHub URL\n",
    "df_clean_2 = load_airbnb_data_from_github(GITHUB_CLEAN_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment_length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "word_count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "e37191f2-9baa-4a4d-9af4-5f48b83dfb87",
       "rows": [
        [
         "count",
         "1000.0",
         "1000.0"
        ],
        [
         "mean",
         "184.719",
         "33.776"
        ],
        [
         "std",
         "269.236312987738",
         "49.06949805775111"
        ],
        [
         "min",
         "3.0",
         "1.0"
        ],
        [
         "25%",
         "47.0",
         "9.0"
        ],
        [
         "50%",
         "100.5",
         "19.0"
        ],
        [
         "75%",
         "216.0",
         "39.0"
        ],
        [
         "max",
         "4390.0",
         "815.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_length</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>184.719000</td>\n",
       "      <td>33.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>269.236313</td>\n",
       "      <td>49.069498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100.500000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>216.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4390.000000</td>\n",
       "      <td>815.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       comment_length   word_count\n",
       "count     1000.000000  1000.000000\n",
       "mean       184.719000    33.776000\n",
       "std        269.236313    49.069498\n",
       "min          3.000000     1.000000\n",
       "25%         47.000000     9.000000\n",
       "50%        100.500000    19.000000\n",
       "75%        216.000000    39.000000\n",
       "max       4390.000000   815.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 22 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   comment_id           1000 non-null   object\n",
      " 1   video_id             1000 non-null   object\n",
      " 2   text                 1000 non-null   object\n",
      " 3   is_toxic             1000 non-null   bool  \n",
      " 4   is_abusive           1000 non-null   bool  \n",
      " 5   is_threat            1000 non-null   bool  \n",
      " 6   is_provocative       1000 non-null   bool  \n",
      " 7   is_obscene           1000 non-null   bool  \n",
      " 8   is_hatespeech        1000 non-null   bool  \n",
      " 9   is_racist            1000 non-null   bool  \n",
      " 10  is_nationalist       1000 non-null   bool  \n",
      " 11  is_sexist            1000 non-null   bool  \n",
      " 12  is_homophobic        1000 non-null   bool  \n",
      " 13  is_religious_hate    1000 non-null   bool  \n",
      " 14  is_radicalism        1000 non-null   bool  \n",
      " 15  comment_length       1000 non-null   int64 \n",
      " 16  word_count           1000 non-null   int64 \n",
      " 17  has_emoji            1000 non-null   bool  \n",
      " 18  has_url              1000 non-null   bool  \n",
      " 19  has_special_char     1000 non-null   bool  \n",
      " 20  is_self_promotional  1000 non-null   bool  \n",
      " 21  contains_tag         1000 non-null   bool  \n",
      "dtypes: bool(17), int64(2), object(3)\n",
      "memory usage: 55.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_clean_2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traducir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load translation model\u001b[39;00m\n\u001b[32m      2\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mHelsinki-NLP/opus-mt-en-es\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m tokenizer = \u001b[43mMarianTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[32m      4\u001b[39m model = MarianMTModel.from_pretrained(model_name)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtranslate_texts\u001b[39m(texts, batch_size=\u001b[32m8\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaelp\\Desktop\\Yael\\Bootcamp_FactoriaF5_IA_Promo4\\Proyectos\\Grupales\\sentiment-analyzer-inc\\.venv_grupo4\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1994\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1993\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1994\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\yaelp\\Desktop\\Yael\\Bootcamp_FactoriaF5_IA_Promo4\\Proyectos\\Grupales\\sentiment-analyzer-inc\\.venv_grupo4\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1980\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1977\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   1979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1980\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Load translation model\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "def translate_texts(texts, batch_size=8):\n",
    "    translated_texts = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        output = model.generate(**tokens)\n",
    "        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in output]\n",
    "        translated_texts.extend(translated_batch)\n",
    "    return translated_texts\n",
    "\n",
    "def augment_with_spanish(df, text_column='text'):\n",
    "    df = df.copy()\n",
    "    spanish_texts = translate_texts(df[text_column].tolist())\n",
    "    \n",
    "    df_translated = df.copy()\n",
    "    df_translated[text_column] = spanish_texts\n",
    "\n",
    "    df_augmented = pd.concat([df, df_translated], ignore_index=True)\n",
    "    return df_augmented\n",
    "\n",
    "df_augmented = augment_with_spanish(df_clean_2, text_column='text')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "# <center> <span style=\"color:#f6f794\"> üíæüíæüíæüíæüíæ Saving in GitHub Compressed üíæüíæüíæüíæüíæ</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Saving data in cleaning_file_path and in GitHub so it can be used in the next steps and the rest of the team can access it\n",
    "# cleaning_file_path = os.path.join(data_dir, \"youtube_dataset_cleaned.csv.gz\")\n",
    "\n",
    "# # Saving the cleaned DataFrame to a compressed CSV file\n",
    "# df_clean_1.to_csv(\n",
    "#     cleaning_file_path,\n",
    "#     index=False,\n",
    "#     compression='gzip',\n",
    "#     sep=',',                  # Crucial: Specify delimiter even for compressed\n",
    "#     encoding='utf-8',         # Crucial: Specify encoding\n",
    "#     quoting=csv.QUOTE_MINIMAL # Crucial: Specify quoting behavior\n",
    "# )\n",
    "# print(f\"‚úÖ Data saved successfully at {cleaning_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_grupo4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
